{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Additions on Long numbers using LongFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports go here\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from long_attention import SliddingWindowAttention\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### **Tokenizer: Organizing the addition:**\n",
    "\n",
    "The tokenizer that we implemented here rearranges the digits in a sum by grouping digits of the same significance together, ordered from least to most significant (right to left). For example, the sum `13 + 54` is encoded as:\n",
    "$$ [3, 4, 1, 5] $$\n",
    "\n",
    "This encoding method aligns well with the Self-Attention Mechanism used in Transformer models. Since the attention score of a token $x[t]$ at position $t$ is computed with respect to all previous tokens (positions $< t$), we want to ensure that the model attends to digits of lower significance before higher ones. This simulates the way humans naturally perform addition.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "eos_token = '[EOS]'\n",
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Binary representation tokenizer\n",
    "    \"\"\"\n",
    "    def __init__(self, number_bits):\n",
    "        self.delimiters = r'(\\[EOS\\]|[,\\+\\=\\s])'\n",
    "        self.vocab = [str(x) for x in range(10)] + [eos_token] + [\"=\"]  # No need for pad token\n",
    "        self.token_to_id = {v : k for k, v in enumerate(self.vocab)}\n",
    "        self.id_to_token = {k : v for k, v in enumerate(self.vocab)}\n",
    "        self.ntokens = len(self.vocab)\n",
    "        self.pattern = f\"[^{re.escape(''.join(self.vocab))}]\"\n",
    "        self.number_bits = number_bits\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Splitting number from symbols\n",
    "        tokens_split = re.split(self.delimiters, text) # Splitting number from symbols\n",
    "\n",
    "        # Keeping only numbers and = symbol\n",
    "        tokens = [token for token in tokens_split if token.isdigit() or token == '=']\n",
    "\n",
    "        if tokens == []:\n",
    "            print(\"Invalid prompt, please use at least one number or the sign =\")\n",
    "            raise ValueError\n",
    "\n",
    "        # get the index of '=' to separate number that should be added vs the answer!\n",
    "        idx_equal = len(tokens)\n",
    "        if '=' in tokens:\n",
    "            idx_equal = tokens.index('=')\n",
    "\n",
    "        # Pad number with 0 in the beginning if they have less than 'number_bits' digits\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i].isdigit():\n",
    "                tokens[i] = '0'*(self.number_bits + 1 - len(tokens[i])) + tokens[i]\n",
    "\n",
    "        # If we have only one token (number or =), then return its encoding directly\n",
    "        if len(tokens) == 1:\n",
    "            return [self.token_to_id[c] for c in tokens[0]]\n",
    "\n",
    "        # Now we are sure that we have all numbers of size self.number_bits + 1\n",
    "        # Let us now put every two digits of the same base 10 position next to each other, starting from the units (unit√©s, puis dixaines, puis centaines,..)\n",
    "        # This ordering (from right to left) is chosen because the attention mechanism considers tokens that were shown in the past\n",
    "        arranged_digits = []\n",
    "        for i in range(self.number_bits + 1):\n",
    "            # Pathological case: do not reverse\n",
    "            if len(tokens[:idx_equal]) == 1:\n",
    "                for token in tokens[:idx_equal]:\n",
    "                    arranged_digits.append(token[i])\n",
    "\n",
    "            else: # reverse\n",
    "                for token in tokens[:idx_equal]:\n",
    "                    arranged_digits.append(token[~i])\n",
    "\n",
    "        # Add the answer now: remaining tokens after idx_equal\n",
    "        for token in tokens[idx_equal:]:\n",
    "            arranged_digits += list(token)\n",
    "\n",
    "        return [self.token_to_id[c] for c in arranged_digits]\n",
    "\n",
    "    def merge_digits(self, l):\n",
    "        result = []\n",
    "        num = \"\"\n",
    "        for char in l:\n",
    "            if char.isdigit():\n",
    "                num += char  # Concatenate digits\n",
    "            else:\n",
    "                if num:  # If there is a collected number, add it to the result\n",
    "                    result.append(str(int(num))) # to remove zeros in the beginning!\n",
    "                    num = \"\"  # Reset num\n",
    "                result.append(char)  # Add the non-digit character\n",
    "\n",
    "        if num:  # Add any remaining number at the end\n",
    "            result.append(str(int(num)))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def decode(self, token_list):\n",
    "        tokens = [self.id_to_token[j] for j in token_list]\n",
    "\n",
    "        m = len(tokens)\n",
    "        if m <= self.number_bits + 2 or self.id_to_token[token_list[-1]] == eos_token:# Answer\n",
    "            l = self.merge_digits(tokens)\n",
    "            return ''.join(l)\n",
    "\n",
    "        else: # It a query\n",
    "            # The number of input numbers for addition\n",
    "            # Take the numbers before the sin equal\n",
    "            idx_equal = len(tokens)\n",
    "            if '=' in tokens:\n",
    "                idx_equal = tokens.index('=')\n",
    "\n",
    "            numbers_before = tokens[:idx_equal]\n",
    "            k = len(numbers_before) // (self.number_bits + 1)\n",
    "\n",
    "            numbers = []\n",
    "            for i in range(k):\n",
    "                num = list(reversed(numbers_before[i::k]))\n",
    "                num = ''.join(num)\n",
    "                num = str(int(num)) # To remove zeros used for padding\n",
    "                numbers.append(num)\n",
    "\n",
    "            text = '+'.join(numbers)\n",
    "\n",
    "            # Now add the numbers after =, i.e the solution\n",
    "            l = self.merge_digits(tokens[idx_equal:])\n",
    "            text = text + ''.join(l)\n",
    "            return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 0, 9, 0, 9, 9, 0, 0, 0, 0, 11, 0, 2, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'999+900=2000'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(4)\n",
    "prompt = \"999 + +  900 = 2000\"\n",
    "inputs = tokenizer.encode(prompt)\n",
    "print(inputs)\n",
    "tokenizer.decode(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we group each pair of digits of the same significance next to each other, \n",
    "# then a good positional encoding could be to give each pair the same positional encoding!\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, repeats = 50, number_bits = 3):\n",
    "        # Create a positional embedding that is periodic in the number of bits used to represent numbers!\n",
    "        # numbers should be consecutive with no double + or double so that this encoding works (i.e clear and clean prompts!)\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.number_bits = number_bits\n",
    "        position = torch.arange(repeats, dtype= torch.float).repeat_interleave(2).unsqueeze(1) # each position is repeated 2 times: [0,0, 1, 1, 2, 2,...]\n",
    "\n",
    "        # Positional encodings\n",
    "        pe = torch.zeros(position.size(0), d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1) # shape (2 * repeats, 1, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from addtions import *\n",
    "dataset_size = 64000\n",
    "number_bits = \n",
    "data = []\n",
    "for _ in range(dataset_size):\n",
    "    data.append(sample_datapoint(number_bits))\n",
    "data[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data[: int(train_proportion * dataset_size)]\n",
    "data_test = data[int(train_proportion * dataset_size):]\n",
    "\n",
    "len(data_train),len(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Longformer\n",
    "\n",
    "Still need the final implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LongFormer()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # disable dropout!\n",
    "\n",
    "prompt = \"2+3=\"\n",
    "prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
    "output = generate(model, prompt_tensor).view((1,-1))\n",
    "output, tokenizer.decode(output.tolist()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(token_list, type_list = \"prompts\"):\n",
    "    max_length = max([len(x) for x in token_list])\n",
    "    out = []\n",
    "    for x in token_list:\n",
    "        if type_list == \"prompts\":\n",
    "            #out.append([tokenizer.token_to_id[pad_token]] * (max_length - len(x)) + x)\n",
    "            #out.append([0] * (max_length - len(x)) + x)\n",
    "            out.append(x)\n",
    "        if type_list == \"answers\":\n",
    "            #out.append(x + [tokenizer.token_to_id[eos_token]] + [tokenizer.token_to_id[pad_token]] * (max_length - len(x)))\n",
    "            #out.append(x + [tokenizer.token_to_id[eos_token]] + [0] * (max_length - len(x)))\n",
    "            out.append(x + [tokenizer.token_to_id[eos_token]])\n",
    "    return out, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [tokenizer.encode(\"1+1=\"), tokenizer.encode(\"21+35=\")]\n",
    "answers = [tokenizer.encode(\"2\"), tokenizer.encode(\"56\")]\n",
    "padded_prompts, _ = pad(prompts, \"prompts\")\n",
    "padded_answers, _ = pad(answers, \"answers\")\n",
    "prompts, padded_answers\n",
    "[tokenizer.decode(p) for p in padded_prompts], [tokenizer.decode(p) for p in padded_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, i):\n",
    "    data = data_train if split == 'train' else data_test\n",
    "    prompts = [tokenizer.encode(data[i][0]) for i in range(i, i + batch_size)]\n",
    "    padded_prompts, length_prompts = pad(prompts, \"prompts\")\n",
    "    answers = [tokenizer.encode(data[i][1]) for i in range(i, i + batch_size)]\n",
    "    padded_answers, length_answers = pad(answers, \"answers\")\n",
    "    X = torch.stack([torch.tensor(x) for x in padded_prompts], 1)\n",
    "    Y = torch.stack([torch.tensor(x) for x in padded_answers], 1)\n",
    "    return X, Y, length_prompts, length_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, length_prompts, length_answers = get_batch(\"train\", 243)\n",
    "X.shape, Y.shape, length_prompts, length_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    # Turn on evaluation mode disables dropout.\n",
    "    model.eval()\n",
    "    correct = 0.\n",
    "    with torch.no_grad():\n",
    "        for batch, i in enumerate(range(0, len(data_test) - 1, batch_size)):\n",
    "            prompts, target_answers, length_prompts, length_answers = get_batch(\"test\", i)\n",
    "            prompts = prompts.to(device) # (length_prompts, batch_size)\n",
    "            target_answers = target_answers.to(device) # (length_answers + 1, batch_size)\n",
    "            output = generate(model, prompts, length_answers + 1) # (length_prompts + length_answers + 1, batch_size)\n",
    "            answers_tokens = output[length_prompts:, :] # (length_answers + 1, batch_size), contains tokens\n",
    "            equality_test = answers_tokens == target_answers # (length_answers + 1, batch_size), contains boolean values\n",
    "            correct += torch.all(equality_test, axis=0).float().sum()\n",
    "        accuracy = correct / len(data_test)\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\n",
    "        prompts, target_answers, length_prompts, length_answers = get_batch(\"train\", i)\n",
    "        prompts = prompts.to(device) # (length_prompts, batch_size)\n",
    "        target_answers = target_answers.to(device) # (length_answers, batch_size)\n",
    "        input_tensor = torch.cat((prompts, target_answers), 0) # (length_prompts + length_answers, batch_size)\n",
    "        model.zero_grad()\n",
    "\n",
    "        # TODO: Add comments to the following 4 lines\n",
    "        output, _ = model(input_tensor) # (length_prompts + length_answers, batch_size, ntokens)\n",
    "        output_answers = output[length_prompts-1:-1,:,:].reshape(-1, ntokens) # (length_answers * batch_size, ntokens)\n",
    "        target_answers = target_answers.view(-1)\n",
    "        loss = F.cross_entropy(output_answers, target_answers)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.2f} | perplexity {:8.2f}'.format(batch, len(data_train) // batch_size,\n",
    "                                                                                                        elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def train(model, epochs):\n",
    "    best_test_accuracy = None\n",
    "    test_accuracy = evaluate(model)\n",
    "    print('-' * 89)\n",
    "    print('| initialisation | test accuracy {:5.2f}'.format(test_accuracy))\n",
    "    print('-' * 89)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train_epoch(model)\n",
    "        test_accuracy = evaluate(model)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}'.format(epoch, (time.time() - epoch_start_time), test_accuracy))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the test accuracy is the best we've seen so far.\n",
    "        if not best_test_accuracy or test_accuracy < best_test_accuracy:\n",
    "            with open(\"arithmetic.pt\", 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_test_accuracy = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load again the model (random or from checkpoint)\n",
    "model = TransformerModel(ntoken = ntokens,\n",
    "                         ninp = 128,\n",
    "                         nhead = 16,\n",
    "                         nhid = 64,\n",
    "                         nlayers = 8)\n",
    "model.to(device)\n",
    "learning_rate = 8e-4\n",
    "epochs = 30\n",
    "train(model, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_examples(model, data_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(20):\n",
    "            prompt, answers = data_test[i]\n",
    "            prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
    "            output = generate(model, prompt_tensor, len(answers) + 1).view((1,-1))\n",
    "            print(tokenizer.decode(output.tolist()[0]) + \"\\t actual result: \" + answers)\n",
    "\n",
    "show_examples(model, data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
