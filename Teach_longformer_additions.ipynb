{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XH77awtshvN"
      },
      "source": [
        "# Performing Additions on Long numbers using LongFormer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "q-cv_PQZshvO"
      },
      "outputs": [],
      "source": [
        "# Imports go here\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from long_attention import *\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BadQ0bVshvO",
        "outputId": "7903cc34-2c9b-41d1-daf2-aba3603b5788"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UuObI7nshvP"
      },
      "source": [
        "# Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD0TgGxZshvP"
      },
      "source": [
        "\n",
        "<div class=\"alert alert-info\">\n",
        "\n",
        "### **Tokenizer: Organizing the addition:**\n",
        "\n",
        "The tokenizer that we implemented here rearranges the digits in a sum by grouping digits of the same significance together, ordered from least to most significant (right to left). For example, the sum `13 + 54` is encoded as:\n",
        "$$ [3, 4, 1, 5] $$\n",
        "\n",
        "This encoding method aligns well with the Self-Attention Mechanism used in Transformer models. Since the attention score of a token $x[t]$ at position $t$ is computed with respect to all previous tokens (positions $< t$), we want to ensure that the model attends to digits of lower significance before higher ones. This simulates the way humans naturally perform addition.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "TrlUOoNSshvP"
      },
      "outputs": [],
      "source": [
        "# Tokenizer\n",
        "eos_token = '[EOS]'\n",
        "class Tokenizer:\n",
        "    \"\"\"\n",
        "    Binary representation tokenizer\n",
        "    \"\"\"\n",
        "    def __init__(self, number_bits):\n",
        "        self.delimiters = r'(\\[EOS\\]|[,\\+\\=\\s])'\n",
        "        self.vocab = [str(x) for x in range(10)] + [eos_token] + [\"=\"]  # No need for pad token\n",
        "        self.token_to_id = {v : k for k, v in enumerate(self.vocab)}\n",
        "        self.id_to_token = {k : v for k, v in enumerate(self.vocab)}\n",
        "        self.ntokens = len(self.vocab)\n",
        "        self.pattern = f\"[^{re.escape(''.join(self.vocab))}]\"\n",
        "        self.number_bits = number_bits\n",
        "        self.vocab_size = len(self.vocab)\n",
        "\n",
        "    def encode(self, text):\n",
        "        # Splitting number from symbols\n",
        "        tokens_split = re.split(self.delimiters, text) # Splitting number from symbols\n",
        "\n",
        "        # Keeping only numbers and = symbol\n",
        "        tokens = [token for token in tokens_split if token.isdigit() or token == '=']\n",
        "\n",
        "        if tokens == []:\n",
        "            print(\"Invalid prompt, please use at least one number or the sign =\")\n",
        "            raise ValueError\n",
        "\n",
        "        # get the index of '=' to separate number that should be added vs the answer!\n",
        "        idx_equal = len(tokens)\n",
        "        if '=' in tokens:\n",
        "            idx_equal = tokens.index('=')\n",
        "\n",
        "        # Pad number with 0 in the beginning if they have less than 'number_bits' digits\n",
        "        for i in range(len(tokens)):\n",
        "            if tokens[i].isdigit():\n",
        "                tokens[i] = '0'*(self.number_bits + 1 - len(tokens[i])) + tokens[i]\n",
        "\n",
        "        # If we have only one token (number or =), then return its encoding directly\n",
        "        if len(tokens) == 1:\n",
        "            return [self.token_to_id[c] for c in tokens[0]]\n",
        "\n",
        "        # Now we are sure that we have all numbers of size self.number_bits + 1\n",
        "        # Let us now put every two digits of the same base 10 position next to each other, starting from the units (unit√©s, puis dixaines, puis centaines,..)\n",
        "        # This ordering (from right to left) is chosen because the attention mechanism considers tokens that were shown in the past\n",
        "        arranged_digits = []\n",
        "        for i in range(self.number_bits + 1):\n",
        "            # Pathological case: do not reverse\n",
        "            if len(tokens[:idx_equal]) == 1:\n",
        "                for token in tokens[:idx_equal]:\n",
        "                    arranged_digits.append(token[i])\n",
        "\n",
        "            else: # reverse\n",
        "                for token in tokens[:idx_equal]:\n",
        "                    arranged_digits.append(token[~i])\n",
        "\n",
        "        # Add the answer now: remaining tokens after idx_equal\n",
        "        for token in tokens[idx_equal:]:\n",
        "            arranged_digits += list(token)\n",
        "\n",
        "        return [self.token_to_id[c] for c in arranged_digits]\n",
        "\n",
        "    def merge_digits(self, l):\n",
        "        result = []\n",
        "        num = \"\"\n",
        "        for char in l:\n",
        "            if char.isdigit():\n",
        "                num += char  # Concatenate digits\n",
        "            else:\n",
        "                if num:  # If there is a collected number, add it to the result\n",
        "                    result.append(str(int(num))) # to remove zeros in the beginning!\n",
        "                    num = \"\"  # Reset num\n",
        "                result.append(char)  # Add the non-digit character\n",
        "\n",
        "        if num:  # Add any remaining number at the end\n",
        "            result.append(str(int(num)))\n",
        "\n",
        "        return result\n",
        "\n",
        "    def decode(self, token_list):\n",
        "        tokens = [self.id_to_token[j] for j in token_list]\n",
        "\n",
        "        m = len(tokens)\n",
        "        if m <= self.number_bits + 2 or self.id_to_token[token_list[-1]] == eos_token:# Answer\n",
        "            l = self.merge_digits(tokens)\n",
        "            return ''.join(l)\n",
        "\n",
        "        else: # It a query\n",
        "            # The number of input numbers for addition\n",
        "            # Take the numbers before the sin equal\n",
        "            idx_equal = len(tokens)\n",
        "            if '=' in tokens:\n",
        "                idx_equal = tokens.index('=')\n",
        "\n",
        "            numbers_before = tokens[:idx_equal]\n",
        "            k = len(numbers_before) // (self.number_bits + 1)\n",
        "\n",
        "            numbers = []\n",
        "            for i in range(k):\n",
        "                num = list(reversed(numbers_before[i::k]))\n",
        "                num = ''.join(num)\n",
        "                num = str(int(num)) # To remove zeros used for padding\n",
        "                numbers.append(num)\n",
        "\n",
        "            text = '+'.join(numbers)\n",
        "\n",
        "            # Now add the numbers after =, i.e the solution\n",
        "            l = self.merge_digits(tokens[idx_equal:])\n",
        "            text = text + ''.join(l)\n",
        "            return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "QvcRnPnBshvQ",
        "outputId": "61b96e80-a412-41d3-efad-d21fbcd29aeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[9, 0, 9, 0, 9, 9, 0, 0, 11, 2, 0, 0, 0]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'999+900=2000'"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "number_bits = 3\n",
        "tokenizer = Tokenizer(number_bits)\n",
        "prompt = \"999 + +  900 = 2000\"\n",
        "inputs = tokenizer.encode(prompt)\n",
        "print(inputs)\n",
        "tokenizer.decode(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDc9Fa9wshvQ"
      },
      "source": [
        "# Positional encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "nY2G2urlshvQ"
      },
      "outputs": [],
      "source": [
        "# Because we group each pair of digits of the same significance next to each other,\n",
        "# then a good positional encoding could be to give each pair the same positional encoding!\n",
        "\n",
        "class Abacus(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, repeats = 50, number_bits = 3):\n",
        "        # Create a positional embedding that is periodic in the number of bits used to represent numbers!\n",
        "        # numbers should be consecutive with no double + or double so that this encoding works (i.e clear and clean prompts!)\n",
        "        super(Abacus, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.number_bits = number_bits\n",
        "        position = torch.arange(repeats, dtype= torch.float).repeat_interleave(2).unsqueeze(1) # each position is repeated 2 times: [0,0, 1, 1, 2, 2,...]\n",
        "\n",
        "        # Positional encodings\n",
        "        pe = torch.zeros(position.size(0), d_model)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(1) # shape (2 * repeats, 1, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        r\"\"\"Inputs of forward function\n",
        "        Args:\n",
        "            x: the sequence fed to the positional encoder model (required).\n",
        "        Shape:\n",
        "            x: [sequence length, batch size, embed dim]\n",
        "            output: [sequence length, batch size, embed dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "7oj-68v5Czvw"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
        "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
        "        Here, we use sine and cosine functions of different frequencies.\n",
        "    .. math:\n",
        "        \\text{PosEmbedder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
        "        \\text{PosEmbedder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
        "        \\text{where pos is the word position and i is the embed idx)\n",
        "    Args:\n",
        "        d_model: the embed dim (required).\n",
        "        dropout: the dropout value (default=0.1).\n",
        "        max_len: the max. length of the incoming sequence (default=5000).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        r\"\"\"Inputs of forward function\n",
        "        Args:\n",
        "            x: the sequence fed to the positional encoder model (required).\n",
        "        Shape:\n",
        "            x: [sequence length, batch size, embed dim]\n",
        "            output: [sequence length, batch size, embed dim]\n",
        "        \"\"\"\n",
        "\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bWvvHIDshvQ"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "gdSPrOzXshvQ"
      },
      "outputs": [],
      "source": [
        "def sample_datapoint(number_bits = 3):\n",
        "    \"\"\"\n",
        "    returns a string containing two random numbers on `number_bits` many bits and their sum.\n",
        "    \"\"\"\n",
        "    a_list = [random.randint(0, 9) for _ in range(number_bits)]\n",
        "    b_list = [random.randint(0, 9) for _ in range(number_bits)]\n",
        "    a_int = int(\"\".join([str(x) for x in a_list]))\n",
        "    b_int = int(\"\".join([str(x) for x in b_list]))\n",
        "    sum_int = a_int + b_int\n",
        "    return (str(a_int) + \"+\" + str(b_int) + \"=\", str(sum_int))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBzyK_TQshvR",
        "outputId": "5d6b2916-1969-4f90-999d-0edd7f84eb56"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('302+160=', '462'),\n",
              " ('505+871=', '1376'),\n",
              " ('997+725=', '1722'),\n",
              " ('926+636=', '1562')]"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_size = 64000\n",
        "number_bits = 3\n",
        "data = []\n",
        "for _ in range(dataset_size):\n",
        "    data.append(sample_datapoint(number_bits))\n",
        "data[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlWcRTKWshvR",
        "outputId": "76d257bb-c016-4825-8ee1-246a69196821"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(57600, 6400)"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_proportion = 0.9\n",
        "data_train = data[: int(train_proportion * dataset_size)]\n",
        "data_test = data[int(train_proportion * dataset_size):]\n",
        "\n",
        "len(data_train),len(data_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECWQMhLvshvR"
      },
      "source": [
        "# Model: Longformer\n",
        "\n",
        "Still need the final implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhCdzGoZshvR",
        "outputId": "85ea1914-3762-4e3f-e972-e68d55436caa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The total number of trainable parameters is  3165708\n"
          ]
        }
      ],
      "source": [
        "# Parameters of the model\n",
        "vocab_size = tokenizer.vocab_size\n",
        "embed_dim = 256\n",
        "n_blocks = 4\n",
        "num_heads = 4\n",
        "window_size = 3\n",
        "\n",
        "# Positional embedder\n",
        "positional_encoder = PositionalEmbedding(embed_dim)\n",
        "\n",
        "# LongFormer\n",
        "model = LongFormer(vocab_size, embed_dim, num_heads, window_size, n_blocks, positional_encoder).to(device)\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"The total number of trainable parameters is \", trainable_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "WL0v0Y8ZshvR"
      },
      "outputs": [],
      "source": [
        "def generate(model, prompts, new_tokens = 5, device = device):\n",
        "    input_tensor = prompts # (batch_size, length_prompts)\n",
        "    input_tensor = input_tensor.to(device)\n",
        "\n",
        "    for _ in range(new_tokens):\n",
        "        output = model(input_tensor) # (batch_size, length_prompts, ntokens)\n",
        "        last_output = output[:,-1,:] # (batch_size, ntokens)\n",
        "        token = torch.argmax(last_output, -1).view(-1, 1) # (batch_size, 1)\n",
        "        input_tensor = torch.cat((input_tensor, token), 1)\n",
        "    return input_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZMyywYfshvR",
        "outputId": "076504c4-8989-4101-c97e-486c0753403c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 9])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor([[ 2,  2,  0,  3,  0,  3,  0,  0, 11,  8,  2,  8,  2,  8]],\n",
              "        device='cuda:0'),\n",
              " '2+332=82828')"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval() # disable dropout!\n",
        "\n",
        "prompt = \"2+332=\"\n",
        "prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((1,-1)).to(device)\n",
        "print(prompt_tensor.shape)\n",
        "output = generate(model, prompt_tensor)\n",
        "output, tokenizer.decode(output.tolist()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVZ9VrtmshvR"
      },
      "source": [
        "# Preprocessing steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "Neyoc7AJshvR"
      },
      "outputs": [],
      "source": [
        "def pad(token_list, type_list = \"prompts\"):\n",
        "    max_length = max([len(x) for x in token_list])\n",
        "    out = []\n",
        "    for x in token_list:\n",
        "        if type_list == \"prompts\":\n",
        "            out.append(x)\n",
        "        if type_list == \"answers\":\n",
        "            out.append(x + [tokenizer.token_to_id[eos_token]])\n",
        "    return out, max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXyPYXQdshvR",
        "outputId": "1d2ab82e-190c-4b11-81e7-a123b0fa6d01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['1+1=', '21+35='], ['2[EOS]', '56[EOS]'])"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompts = [tokenizer.encode(\"1+1=\"), tokenizer.encode(\"21+35=\")]\n",
        "answers = [tokenizer.encode(\"2\"), tokenizer.encode(\"56\")]\n",
        "padded_prompts, _ = pad(prompts, \"prompts\")\n",
        "padded_answers, _ = pad(answers, \"answers\")\n",
        "prompts, padded_answers\n",
        "[tokenizer.decode(p) for p in padded_prompts], [tokenizer.decode(p) for p in padded_answers]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "-crcNh5MshvR"
      },
      "outputs": [],
      "source": [
        "def get_batch(split, i, batch_size):\n",
        "    data = data_train if split == 'train' else data_test\n",
        "    prompts = [tokenizer.encode(data[i][0]) for i in range(i, i + batch_size)]\n",
        "    padded_prompts, length_prompts = pad(prompts, \"prompts\")\n",
        "    answers = [tokenizer.encode(data[i][1]) for i in range(i, i + batch_size)]\n",
        "    padded_answers, length_answers = pad(answers, \"answers\")\n",
        "    X = torch.stack([torch.tensor(x) for x in padded_prompts], 1)\n",
        "    Y = torch.stack([torch.tensor(x) for x in padded_answers], 1)\n",
        "    return X.T, Y.T, length_prompts, length_answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VAvLa1oshvS",
        "outputId": "7c15eb5e-e114-4550-e8cb-63c03c518242"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([64, 9]), torch.Size([64, 5]), 9, 4)"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X, Y, length_prompts, length_answers = get_batch(\"train\", 243, 64)\n",
        "X.shape, Y.shape, length_prompts, length_answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12lNEkKAshvS",
        "outputId": "b619053a-4c46-40a2-844a-773ffdae68b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 0,  9,  3,  3, 10])\n"
          ]
        }
      ],
      "source": [
        "print(Y[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuanKIQCUqkk",
        "outputId": "267b8afd-c12f-45a9-84b9-edbf2d1ce83d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n"
          ]
        }
      ],
      "source": [
        "print(length_answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "nB1ozvRLshvS"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, batch_size = 64):\n",
        "    # Turn on evaluation mode disables dropout.\n",
        "    model.eval()\n",
        "    correct = 0.\n",
        "    with torch.no_grad():\n",
        "        for batch, i in enumerate(range(0, len(data_test) - 1, batch_size)):\n",
        "            prompts, target_answers, length_prompts, length_answers = get_batch(\"test\", i, batch_size)\n",
        "            prompts = prompts.to(device) # (batch_size, length_prompts)\n",
        "            target_answers = target_answers.to(device) # (batch_size, length_answers + 1)\n",
        "            output = generate(model, prompts, length_answers + 1) # (batch_size, length_prompts + length_answers + 1)\n",
        "            answers_tokens = output[:, length_prompts:] # (batch_size, length_answers + 1), contains tokens\n",
        "            equality_test = answers_tokens == target_answers # (batch_size, length_answers + 1), contains boolean values\n",
        "            correct += torch.all(equality_test, axis=1).float().sum()\n",
        "        accuracy = correct / len(data_test)\n",
        "    return accuracy.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vBc9LVvshvS",
        "outputId": "4921a46e-abbd-419f-bb22-4c115677fd97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1MfXziHXGSV",
        "outputId": "80c9c800-a920-4992-9a34-e31fe7c7122f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 5])\n",
            "torch.Size([160])\n"
          ]
        }
      ],
      "source": [
        "prompts, target_answers, length_prompts, length_answers = get_batch(\"train\", 0, 32)\n",
        "print(target_answers.shape)\n",
        "target_answers = target_answers.reshape(-1)\n",
        "print(target_answers.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6UcF2jWshvS"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "h-zZ-t5LshvS"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, params, vocab_size):\n",
        "    model.train()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=params['lr'])#, betas = (params['beta_1'], params['beta_2']))\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    batch_size = params['batch_size']\n",
        "\n",
        "    for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\n",
        "        prompts, target_answers, length_prompts, length_answers = get_batch(\"train\", i, batch_size)\n",
        "        prompts = prompts.to(device) # (batch_size, length_prompts)\n",
        "        target_answers = target_answers.to(device) # (batch_size, length_answers)\n",
        "        input_tensor = torch.cat((prompts, target_answers), 1) # (batch_size, length_prompts + length_answers)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(input_tensor) # (batch_size, length_prompts + length_answers, ntokens)\n",
        "        #output_answers = output[:,length_prompts-1:-1,:].reshape(-1, vocab_size) # (length_answers * batch_size, ntokens)\n",
        "        output_answers = output[:,length_prompts:,:].reshape(-1, vocab_size)\n",
        "        target_answers = target_answers.reshape(-1)\n",
        "        loss = F.cross_entropy(output_answers, target_answers)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % params['log_interval'] == 0 and batch > 0:\n",
        "            cur_loss = total_loss / params['log_interval']\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.4f} | perplexity {:8.4f}'.format(batch, len(data_train) // batch_size,\n",
        "                                                                                                        elapsed * 1000 / params['log_interval'], cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def train(model, params, vocab_size):\n",
        "    best_test_accuracy = None\n",
        "    test_accuracy = evaluate(model)\n",
        "    print('-' * 89)\n",
        "    print('| initialisation | test accuracy {:5.2f}'.format(test_accuracy))\n",
        "    print('-' * 89)\n",
        "    for epoch in range(1, params['epochs'] +1):\n",
        "        epoch_start_time = time.time()\n",
        "        train_epoch(model, params, vocab_size)\n",
        "        test_accuracy = evaluate(model)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}'.format(epoch, (time.time() - epoch_start_time), test_accuracy))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the test accuracy is the best we've seen so far.\n",
        "        if not best_test_accuracy or test_accuracy < best_test_accuracy:\n",
        "            with open(\"arithmetic.pt\", 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_test_accuracy = test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "uLY7MfizshvS"
      },
      "outputs": [],
      "source": [
        "# Training params\n",
        "params = {'lr': 8e-4,\n",
        "          'epochs': 5,\n",
        "          #'beta_1': 0.9,\n",
        "          #'beta_2': 0.999,\n",
        "          'batch_size': 64,\n",
        "          'log_interval': 200}\n",
        "vocab_size = tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lb3m3NPnshvS",
        "outputId": "5e80691d-0ac2-4c21-a08c-580620dedbd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The total number of trainable parameters is  796428\n"
          ]
        }
      ],
      "source": [
        "# Load again the model (random or from checkpoint)\n",
        "# Parameters of the model\n",
        "vocab_size = tokenizer.vocab_size\n",
        "embed_dim = 128\n",
        "n_blocks = 4\n",
        "num_heads = 1\n",
        "window_size = 3\n",
        "\n",
        "# Positional embedder\n",
        "positional_encoder = PositionalEmbedding(embed_dim)\n",
        "\n",
        "# LongFormer\n",
        "model = LongFormer(vocab_size, embed_dim, num_heads, window_size, n_blocks, positional_encoder)\n",
        "model.to(device)\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"The total number of trainable parameters is \", trainable_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3K7i1N_ashvS",
        "outputId": "3bf2075d-addf-4199-84aa-dbd3475b6773"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| initialisation | test accuracy  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   200/  900 batches | ms/batch 98.57 | loss 0.0514 | perplexity   1.0527\n",
            "|   400/  900 batches | ms/batch 101.60 | loss 0.0014 | perplexity   1.0014\n",
            "|   600/  900 batches | ms/batch 103.93 | loss 0.0007 | perplexity   1.0007\n",
            "|   800/  900 batches | ms/batch 97.13 | loss 0.0004 | perplexity   1.0004\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 101.36s | test accuracy  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   200/  900 batches | ms/batch 101.88 | loss 0.0000 | perplexity   1.0000\n",
            "|   400/  900 batches | ms/batch 98.32 | loss 0.0000 | perplexity   1.0000\n",
            "|   600/  900 batches | ms/batch 100.56 | loss 0.0000 | perplexity   1.0000\n",
            "|   800/  900 batches | ms/batch 97.34 | loss 0.0000 | perplexity   1.0000\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 101.34s | test accuracy  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   200/  900 batches | ms/batch 104.10 | loss 0.0000 | perplexity   1.0000\n",
            "|   400/  900 batches | ms/batch 103.05 | loss 0.0000 | perplexity   1.0000\n",
            "|   600/  900 batches | ms/batch 102.11 | loss 0.0000 | perplexity   1.0000\n",
            "|   800/  900 batches | ms/batch 104.98 | loss 0.0000 | perplexity   1.0000\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 104.65s | test accuracy  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   200/  900 batches | ms/batch 102.45 | loss 0.0000 | perplexity   1.0000\n",
            "|   400/  900 batches | ms/batch 105.44 | loss 0.0000 | perplexity   1.0000\n",
            "|   600/  900 batches | ms/batch 103.37 | loss 0.0000 | perplexity   1.0000\n",
            "|   800/  900 batches | ms/batch 99.54 | loss 0.0000 | perplexity   1.0000\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 103.97s | test accuracy  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   200/  900 batches | ms/batch 103.12 | loss 0.0000 | perplexity   1.0000\n",
            "|   400/  900 batches | ms/batch 100.59 | loss 0.0000 | perplexity   1.0000\n",
            "|   600/  900 batches | ms/batch 103.60 | loss 0.0000 | perplexity   1.0000\n",
            "|   800/  900 batches | ms/batch 102.68 | loss 0.0000 | perplexity   1.0000\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 102.49s | test accuracy  0.00\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "train(model, params, vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzfUC3i7shvS"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISLamBKlshvS",
        "outputId": "fdd6cf89-bc2d-4ca2-a1a2-1bf2b6d40bca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "64+875=1111\t actual result: 939\n",
            "422+541=1111\t actual result: 963\n",
            "443+479=1111\t actual result: 922\n",
            "902+434=11111\t actual result: 1336\n",
            "974+79=11111\t actual result: 1053\n",
            "13+80=111\t actual result: 93\n",
            "775+883=11111\t actual result: 1658\n",
            "431+792=11111\t actual result: 1223\n",
            "785+53=1111\t actual result: 838\n",
            "386+988=11111\t actual result: 1374\n",
            "220+13=1111\t actual result: 233\n",
            "480+639=11111\t actual result: 1119\n",
            "929+667=11111\t actual result: 1596\n",
            "113+980=11111\t actual result: 1093\n",
            "145+374=1111\t actual result: 519\n",
            "681+773=11111\t actual result: 1454\n",
            "179+336=1111\t actual result: 515\n",
            "453+741=11111\t actual result: 1194\n",
            "753+450=11111\t actual result: 1203\n",
            "434+290=1111\t actual result: 724\n"
          ]
        }
      ],
      "source": [
        "def show_examples(model, data_test):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(20):\n",
        "            prompt, answers = data_test[i]\n",
        "            prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((1,-1)).to(device) # shape (1, length_prompt)\n",
        "            output = generate(model, prompt_tensor, len(answers) + 1) # shape (1, length_prompt+ length_answer)\n",
        "            print(tokenizer.decode(output.tolist()[0]) + \"\\t actual result: \" + answers)\n",
        "\n",
        "show_examples(model, data_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9OTd5aqshvT"
      },
      "source": [
        "----"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
